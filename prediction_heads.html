

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Prediction Heads &mdash; adapter-transformers  documentation</title>
  

  
  
    <link rel="shortcut icon" href="_static/favicon.png"/>
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Embeddings" href="embeddings.html" />
    <link rel="prev" title="Adapter Activation and Composition" href="adapter_composition.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> adapter-transformers
          

          
            
            <img src="_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="training.html">Adapter Training</a></li>
</ul>
<p class="caption"><span class="caption-text">Adapter Methods</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview and Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="methods.html">Adapter Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="method_combinations.html">Method Combinations</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="adapter_composition.html">Adapter Activation and Composition</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Prediction Heads</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#adaptermodel-classes">AdapterModel classes</a></li>
<li class="toctree-l2"><a class="reference internal" href="#model-classes-with-static-heads-huggingface-transformers">Model classes with static heads (HuggingFace Transformers)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#automatic-conversion">Automatic conversion</a></li>
<li class="toctree-l2"><a class="reference internal" href="#custom-heads">Custom Heads</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="embeddings.html">Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="extending.html">Extending the Library</a></li>
<li class="toctree-l1"><a class="reference internal" href="transitioning.html">Transitioning from Earlier Versions</a></li>
</ul>
<p class="caption"><span class="caption-text">Loading and Sharing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="loading.html">Loading Pre-Trained Adapters</a></li>
<li class="toctree-l1"><a class="reference internal" href="hub_contributing.html">Contributing Adapters to the Hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="huggingface_hub.html">Integration with HuggingFaceâ€™s Model Hub</a></li>
</ul>
<p class="caption"><span class="caption-text">Supported Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="model_overview.html">Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/auto.html">Auto Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/bart.html">BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/beit.html">Bidirectional Encoder representation from Image Transformers (BEiT)</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/bert-generation.html">BertGeneration</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/clip.html">CLIP</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/deberta.html">DeBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/deberta_v2.html">DeBERTa-v2</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/encoderdecoder.html">Encoder Decoder Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/gptj.html">EleutherAI GPT-J-6B</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/mbart.html">MBart</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/t5.html">T5</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/vit.html">Vision Transformer (ViT)</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/xlmroberta.html">XLM-RoBERTa</a></li>
</ul>
<p class="caption"><span class="caption-text">Adapter-Related Classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="classes/adapter_config.html">Adapter Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/model_adapters_config.html">Model Adapters Config</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/adapter_modules.html">Adapter Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/adapter_layer.html">AdapterLayer</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/model_mixins.html">Model Mixins</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/adapter_training.html">Adapter Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/adapter_utils.html">Adapter Utilities</a></li>
</ul>
<p class="caption"><span class="caption-text">Contributing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing to AdapterHub</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing/adding_adapter_methods.html">Adding Adapter Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing/adding_adapters_to_a_model.html">Adding Adapters to a Model</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">adapter-transformers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Prediction Heads</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/prediction_heads.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="tex2jax_ignore mathjax_ignore section" id="prediction-heads">
<h1>Prediction Heads<a class="headerlink" href="#prediction-heads" title="Permalink to this headline">Â¶</a></h1>
<p>This section gives an overview how different prediction heads can be used together with adapter modules and how pre-trained adapters can be distributed side-by-side with matching prediction heads in AdapterHub.
We will take a look at the <code class="docutils literal notranslate"><span class="pre">AdapterModel</span></code> classes (e.g. <code class="docutils literal notranslate"><span class="pre">BertAdapterModel</span></code>) introduced by adapter-transformers, which provide <strong>flexible</strong> support for prediction heads, as well as models with <strong>static</strong> heads provided out-of-the-box by HuggingFace Transformers (e.g. <code class="docutils literal notranslate"><span class="pre">BertForSequenceClassification</span></code>).</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>We recommend to use the <a class="reference external" href="#adaptermodel-classes">AdapterModel classes</a> whenever possible.
They have been created specifically for working with adapters and provide more flexibility.</p>
</div>
<div class="section" id="adaptermodel-classes">
<h2>AdapterModel classes<a class="headerlink" href="#adaptermodel-classes" title="Permalink to this headline">Â¶</a></h2>
<p>The AdapterModel classes provided by <code class="docutils literal notranslate"><span class="pre">adapter-transformers</span></code> allow a flexible configuration of prediction heads on top of a pre-trained language model.</p>
<p>First, we load pre-trained model from the HuggingFace Hub via the <a class="reference internal" href="classes/models/auto.html#transformers.adapters.AutoAdapterModel" title="transformers.adapters.AutoAdapterModel"><span class="xref myst py py-class"><code class="docutils literal notranslate"><span class="pre">AutoAdapterModel</span></code></span></a> class:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoAdapterModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>By default, this model doesnâ€™t have any heads yet. We add a new one in the next step:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add_classification_head</span><span class="p">(</span><span class="s2">&quot;mrpc&quot;</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>The line above adds a binary sequence classification head on top of our model.
As this head is named, we could add multiple other heads with different names to the same model.
This is especially useful if used together with matching adapter modules.
To learn more about the different head types and the configuration options, please refer to the class references of the respective model classes, e.g. <a class="reference internal" href="classes/models/bert.html#transformers.adapters.BertAdapterModel" title="transformers.adapters.BertAdapterModel"><span class="xref myst py py-class"><code class="docutils literal notranslate"><span class="pre">BertAdapterModel</span></code></span></a>.</p>
<p>Now, of course, we would like to train our classification head together with an adapter, so letâ€™s add one:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add_adapter</span><span class="p">(</span><span class="s2">&quot;mrpc&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="s2">&quot;pfeiffer&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">set_active_adapters</span><span class="p">(</span><span class="s2">&quot;mrpc&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Since we gave the task adapter the same name as our head, we can easily identify them as belonging together.
The call to <code class="docutils literal notranslate"><span class="pre">set_active_adapters()</span></code> in the second line tells our model to use the adapter - head configuration we specified by default in a forward pass.
At this point, we can start to <a class="reference internal" href="training.html"><span class="doc std std-doc">train our setup</span></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">set_active_adapters()</span></code> will search for an adapter and a prediction head with the given name to be activated.
Alternatively, prediction heads can also be activated explicitly (i.e. without adapter modules).
These three options are possible (in order of priority when multiple are specified):</p>
<ol class="arabic simple">
<li><p>If <code class="docutils literal notranslate"><span class="pre">head</span></code> is passed to the forward call, the head with the given name is used.</p></li>
<li><p>If the forward call is executed within an <code class="docutils literal notranslate"><span class="pre">AdapterSetup</span></code> context, the head configuration is read from the context.</p></li>
<li><p>If the <code class="docutils literal notranslate"><span class="pre">active_head</span></code> property is set, the head configuration is read from there.</p></li>
</ol>
</div>
<p>After training has completed, we can save our whole setup (adapter module <em>and</em> prediction head), with a single call:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">save_adapter</span><span class="p">(</span><span class="s2">&quot;/path/to/dir&quot;</span><span class="p">,</span> <span class="s2">&quot;mrpc&quot;</span><span class="p">,</span> <span class="n">with_head</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Now, you just have to <a class="reference internal" href="hub_contributing.html#add-your-pre-trained-adapter"><span class="std std-doc">share your work with the world</span></a>.
After you published our adapter together with its head in the Hub, anyone else can load both adapter and head by using the same model class.</p>
<p>Alternatively, we can also save and load the prediction head separately from an adapter module:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># save</span>
<span class="n">model</span><span class="o">.</span><span class="n">save_head</span><span class="p">(</span><span class="s2">&quot;/path/to/dir&quot;</span><span class="p">,</span> <span class="s2">&quot;mrpc&quot;</span><span class="p">)</span>
<span class="c1"># load</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_head</span><span class="p">(</span><span class="s2">&quot;/path/to/dir&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Lastly, itâ€™s also possible to delete an added head again:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">delete_head</span><span class="p">(</span><span class="s2">&quot;mrpc&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="model-classes-with-static-heads-huggingface-transformers">
<h2>Model classes with static heads (HuggingFace Transformers)<a class="headerlink" href="#model-classes-with-static-heads-huggingface-transformers" title="Permalink to this headline">Â¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">transformers</span></code> library provides strongly typed model classes with heads for various different tasks (e.g. <code class="docutils literal notranslate"><span class="pre">RobertaForSequenceClassification</span></code>, <code class="docutils literal notranslate"><span class="pre">AutoModelForMultipleChoice</span></code> â€¦).
If an adapter module is trained with one these out-of-the-box classes, it is encouraged to also distribute the prediction head weights together with the adapter weights.
Therefore, we can also easily save the prediction head weights for these models together with an adapter:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">save_adapter</span><span class="p">(</span><span class="s2">&quot;/path/to/dir&quot;</span><span class="p">,</span> <span class="s2">&quot;mrpc&quot;</span><span class="p">,</span> <span class="n">with_head</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>In the next step, we can provide both the adapter weights and the head weights to the Hub.
If someone else then downloads our pre-trained adapter, the resolving method will check if our prediction head matches the class of his model.
In case the classes match, our prediction head weights will be automatically loaded too.</p>
</div>
<div class="section" id="automatic-conversion">
<h2>Automatic conversion<a class="headerlink" href="#automatic-conversion" title="Permalink to this headline">Â¶</a></h2>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Although the two prediction head implementations serve the same use case, their weights are <em>not</em> directly compatible, i.e. you cannot load a head created with <code class="docutils literal notranslate"><span class="pre">AutoAdapterModel</span></code> into a model of type <code class="docutils literal notranslate"><span class="pre">AutoModelForSequenceClassification</span></code>.
There is however an automatic conversion to model classes with flexible heads.</p>
</div>
<p>Beginning with v2.1 of <code class="docutils literal notranslate"><span class="pre">adapter-transformers</span></code>, it is possible to load static heads, e.g. created with <code class="docutils literal notranslate"><span class="pre">AutoModelForSequenceClassification</span></code>, into model classes with flexible heads, e.g. <code class="docutils literal notranslate"><span class="pre">AutoAdapterModel</span></code>.
The conversion of weights happens automatically during the call of <code class="docutils literal notranslate"><span class="pre">load_adapter()</span></code>, so no additional steps are needed:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">static_head_model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">)</span>
<span class="n">static_head_model</span><span class="o">.</span><span class="n">add_adapter</span><span class="p">(</span><span class="s2">&quot;test&quot;</span><span class="p">)</span>
<span class="n">static_head_model</span><span class="o">.</span><span class="n">save_adapter</span><span class="p">(</span><span class="n">temp_dir</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">)</span>

<span class="n">flex_head_model</span> <span class="o">=</span> <span class="n">AutoAdapterModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">)</span>
<span class="n">flex_head_model</span><span class="o">.</span><span class="n">load_adapter</span><span class="p">(</span><span class="n">temp_dir</span><span class="p">)</span>

<span class="k">assert</span> <span class="s2">&quot;test&quot;</span> <span class="ow">in</span> <span class="n">flex_head_model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">adapters</span>
<span class="k">assert</span> <span class="s2">&quot;test&quot;</span> <span class="ow">in</span> <span class="n">flex_head_model</span><span class="o">.</span><span class="n">heads</span>
</pre></div>
</div>
<p>Note that a conversion in the opposite direction is not supported.</p>
</div>
<div class="section" id="custom-heads">
<h2>Custom Heads<a class="headerlink" href="#custom-heads" title="Permalink to this headline">Â¶</a></h2>
<p>If none of the available prediction heads fit your requirements, you can define and add a custom head.</p>
<p>First, we need to define the new head class. For that, the initialization and the forward pass need to be implemented.
The initialization of the head gets a reference to the model, the name of the head, and additionally defined kwargs.
You can use the following template as a guideline.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">CustomHead</span><span class="p">(</span><span class="n">PredictionHead</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">head_name</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># innitialization of the custom head</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">cls_output</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># implementation of the forward pass</span>
</pre></div>
</div>
<p>Next, we can register the new custom head and give the new head type a name. This only notifies
the model that there is a new head type. Then, we can add an instance of the new head to the model by
calling <code class="docutils literal notranslate"><span class="pre">add_custom_head</span></code> with the name of the new head type, the name of the head instance we are creating, and
additional arguments required by the head.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">register_custom_head</span><span class="p">(</span><span class="s2">&quot;my_custom_head&quot;</span><span class="p">,</span> <span class="n">CustomHead</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_custom_head</span><span class="p">(</span><span class="n">head_type</span><span class="o">=</span><span class="s2">&quot;my_custom_head&quot;</span><span class="p">,</span> <span class="n">head_name</span><span class="o">=</span><span class="s2">&quot;custom_head&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
<p>After adding the custom head you can treat it like any other build-in head type.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="embeddings.html" class="btn btn-neutral float-right" title="Embeddings" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="adapter_composition.html" class="btn btn-neutral float-left" title="Adapter Activation and Composition" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020-2022, Adapter-Hub Team

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  <!--- IMPORTANT: This file has modifications compared to the snippet on the documentation page! -->
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Versions</span>
    v: master
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Branches</dt>
      <dd><a href="prediction_heads.html">master</a></dd>
    </dl>
  </div>
</div>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>