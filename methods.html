

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Adapter Methods &mdash; adapter-transformers  documentation</title>
  

  
  
    <link rel="shortcut icon" href="_static/favicon.png"/>
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Method Combinations" href="method_combinations.html" />
    <link rel="prev" title="Overview and Configuration" href="overview.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> adapter-transformers
          

          
            
            <img src="_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="training.html">Adapter Training</a></li>
</ul>
<p class="caption"><span class="caption-text">Adapter Methods</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview and Configuration</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Adapter Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#bottleneck-adapters">Bottleneck Adapters</a></li>
<li class="toctree-l2"><a class="reference internal" href="#language-adapters-invertible-adapters">Language Adapters - Invertible Adapters</a></li>
<li class="toctree-l2"><a class="reference internal" href="#prefix-tuning">Prefix Tuning</a></li>
<li class="toctree-l2"><a class="reference internal" href="#compacter">Compacter</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lora">LoRA</a></li>
<li class="toctree-l2"><a class="reference internal" href="#ia-3">(IA)^3</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="method_combinations.html">Method Combinations</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="adapter_composition.html">Adapter Activation and Composition</a></li>
<li class="toctree-l1"><a class="reference internal" href="prediction_heads.html">Prediction Heads</a></li>
<li class="toctree-l1"><a class="reference internal" href="embeddings.html">Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="extending.html">Extending the Library</a></li>
<li class="toctree-l1"><a class="reference internal" href="transitioning.html">Transitioning from Earlier Versions</a></li>
</ul>
<p class="caption"><span class="caption-text">Loading and Sharing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="loading.html">Loading Pre-Trained Adapters</a></li>
<li class="toctree-l1"><a class="reference internal" href="hub_contributing.html">Contributing Adapters to the Hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="huggingface_hub.html">Integration with HuggingFace’s Model Hub</a></li>
</ul>
<p class="caption"><span class="caption-text">Supported Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="model_overview.html">Model Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/albert.html">ALBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/auto.html">Auto Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/bart.html">BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/beit.html">Bidirectional Encoder representation from Image Transformers (BEiT)</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/bert-generation.html">BertGeneration</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/clip.html">CLIP</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/deberta.html">DeBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/deberta_v2.html">DeBERTa-v2</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/encoderdecoder.html">Encoder Decoder Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/gptj.html">EleutherAI GPT-J-6B</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/mbart.html">MBart</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/roberta.html">RoBERTa</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/t5.html">T5</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/vit.html">Vision Transformer (ViT)</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/models/xlmroberta.html">XLM-RoBERTa</a></li>
</ul>
<p class="caption"><span class="caption-text">Adapter-Related Classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="classes/adapter_config.html">Adapter Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/model_adapters_config.html">Model Adapters Config</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/adapter_modules.html">Adapter Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/adapter_layer.html">AdapterLayer</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/model_mixins.html">Model Mixins</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/adapter_training.html">Adapter Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="classes/adapter_utils.html">Adapter Utilities</a></li>
</ul>
<p class="caption"><span class="caption-text">Contributing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing to AdapterHub</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing/adding_adapter_methods.html">Adding Adapter Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing/adding_adapters_to_a_model.html">Adding Adapters to a Model</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">adapter-transformers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Adapter Methods</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/methods.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="tex2jax_ignore mathjax_ignore section" id="adapter-methods">
<h1>Adapter Methods<a class="headerlink" href="#adapter-methods" title="Permalink to this headline">¶</a></h1>
<p>On this page, we present all adapter methods currently integrated into the <code class="docutils literal notranslate"><span class="pre">adapter-transformers</span></code> library.
A tabular overview of adapter methods is provided <span class="xref myst">here</span>.
Additionally, options to combine multiple adapter methods in a single setup are presented <a class="reference internal" href="method_combinations.html"><span class="doc std std-doc">on the next page</span></a>.</p>
<div class="section" id="bottleneck-adapters">
<h2>Bottleneck Adapters<a class="headerlink" href="#bottleneck-adapters" title="Permalink to this headline">¶</a></h2>
<p><em>Configuration class</em>: <a class="reference internal" href="classes/adapter_config.html#transformers.AdapterConfig" title="transformers.AdapterConfig"><span class="xref myst py py-class"><code class="docutils literal notranslate"><span class="pre">AdapterConfig</span></code></span></a></p>
<p>Bottleneck adapters introduce bottleneck feed-forward layers in each layer of a Transformer model.
Generally, these adapter layers consist of a down-projection matrix <span class="math notranslate nohighlight">\(W_{down}\)</span> that projects the layer hidden states into a lower dimension <span class="math notranslate nohighlight">\(d_{bottleneck}\)</span>, a non-linearity <span class="math notranslate nohighlight">\(f\)</span>, an up-projection <span class="math notranslate nohighlight">\(W_{up}\)</span> that projects back into the original hidden layer dimension and a residual connection <span class="math notranslate nohighlight">\(r\)</span>:</p>
<div class="math notranslate nohighlight">
\[
h \leftarrow W_{up} \cdot f(W_{down} \cdot h) + r
\]</div>
<p>Depending on the concrete adapter configuration, these layers can be introduced at different locations within a Transformer block. Further, residual connections, layer norms, activation functions and bottleneck sizes ,etc., can be configured.</p>
<p>The most important configuration hyperparameter to be highlighted here is the bottleneck dimension <span class="math notranslate nohighlight">\(d_{bottleneck}\)</span>.
In adapter-transformers, this bottleneck dimension is specified indirectly via the <code class="docutils literal notranslate"><span class="pre">reduction_factor</span></code> attribute of a configuration.
This <code class="docutils literal notranslate"><span class="pre">reduction_factor</span></code> defines the ratio between a model’s layer hidden dimension and the bottleneck dimension, i.e.:</p>
<div class="math notranslate nohighlight">
\[
\text{reduction_factor} = \frac{d_{hidden}}{d_{bottleneck}}
\]</div>
<p>A visualization of further configuration options related to the adapter structure is given in the figure below. For more details, we refer to the documentation of <a class="reference internal" href="classes/adapter_config.html#transformers.AdapterConfig" title="transformers.AdapterConfig"><span class="xref myst py py-class"><code class="docutils literal notranslate"><span class="pre">AdapterConfig</span></code></span></a>.</p>
<div class="figure align-center" id="id1">
<a class="reference internal image-reference" href="_images/architecture.png"><img alt="Adapter architectures" src="_images/architecture.png" style="width: 350px;" /></a>
<p class="caption"><span class="caption-text">Visualization of possible adapter configurations with corresponding dictionary keys.</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</div>
<p><code class="docutils literal notranslate"><span class="pre">adapter-transformers</span></code> comes with pre-defined configurations for some bottleneck adapter architectures proposed in literature:</p>
<ul class="simple">
<li><p><a class="reference internal" href="classes/adapter_config.html#transformers.HoulsbyConfig" title="transformers.HoulsbyConfig"><span class="xref myst py py-class"><code class="docutils literal notranslate"><span class="pre">HoulsbyConfig</span></code></span></a>, as proposed by <a class="reference external" href="https://arxiv.org/pdf/1902.00751.pdf">Houlsby et al. (2019)</a>, places adapter layers after both the multi-head attention and feed-forward block in each Transformer layer.</p></li>
<li><p><a class="reference internal" href="classes/adapter_config.html#transformers.PfeifferConfig" title="transformers.PfeifferConfig"><span class="xref myst py py-class"><code class="docutils literal notranslate"><span class="pre">PfeifferConfig</span></code></span></a>, as proposed by <a class="reference external" href="https://arxiv.org/pdf/2005.00052.pdf">Pfeiffer et al. (2020)</a>, places an adapter layer only after the feed-forward block in each Transformer layer.</p></li>
<li><p><a class="reference internal" href="classes/adapter_config.html#transformers.ParallelConfig" title="transformers.ParallelConfig"><span class="xref myst py py-class"><code class="docutils literal notranslate"><span class="pre">ParallelConfig</span></code></span></a>, as proposed by <a class="reference external" href="https://arxiv.org/pdf/2110.04366.pdf">He et al. (2021)</a>, places adapter layers in parallel to the original Transformer layers.</p></li>
</ul>
<p><em>Example</em>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers.adapters</span> <span class="kn">import</span> <span class="n">AdapterConfig</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">AdapterConfig</span><span class="p">(</span><span class="n">mh_adapter</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">output_adapter</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduction_factor</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">non_linearity</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_adapter</span><span class="p">(</span><span class="s2">&quot;bottleneck_adapter&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
<p><em>Papers:</em></p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/pdf/1902.00751.pdf">Parameter-Efficient Transfer Learning for NLP</a> (Houlsby et al., 2019)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1909.08478.pdf">Simple, Scalable Adaptation for Neural Machine Translation</a> (Bapna and Firat, 2019)</p></li>
<li><p><a class="reference external" href="https://aclanthology.org/2021.eacl-main.39.pdf">AdapterFusion: Non-Destructive Task Composition for Transfer Learning</a> (Pfeiffer et al., 2021)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/2007.07779.pdf">AdapterHub: A Framework for Adapting Transformers</a> (Pfeiffer et al., 2020)</p></li>
</ul>
</div>
<div class="section" id="language-adapters-invertible-adapters">
<h2>Language Adapters - Invertible Adapters<a class="headerlink" href="#language-adapters-invertible-adapters" title="Permalink to this headline">¶</a></h2>
<p><em>Configuration class</em>: <a class="reference internal" href="classes/adapter_config.html#transformers.PfeifferInvConfig" title="transformers.PfeifferInvConfig"><span class="xref myst py py-class"><code class="docutils literal notranslate"><span class="pre">PfeifferInvConfig</span></code></span></a>, <a class="reference internal" href="classes/adapter_config.html#transformers.HoulsbyInvConfig" title="transformers.HoulsbyInvConfig"><span class="xref myst py py-class"><code class="docutils literal notranslate"><span class="pre">HoulsbyInvConfig</span></code></span></a></p>
<p>The MAD-X setup (<a class="reference external" href="https://arxiv.org/pdf/2005.00052.pdf">Pfeiffer et al., 2020</a>) proposes language adapters to learn language-specific transformations.
After being trained on a language modeling task, a language adapter can be stacked before a task adapter for training on a downstream task.
To perform zero-shot cross-lingual transfer, one language adapter can simply be replaced by another.</p>
<p>In terms of architecture, language adapters are largely similar to regular bottleneck adapters, except for an additional <em>invertible adapter</em> layer after the LM embedding layer.
Embedding outputs are passed through this invertible adapter in the forward direction before entering the first Transformer layer and in the inverse direction after leaving the last Transformer layer.
Invertible adapter architectures are further detailed in <a class="reference external" href="https://arxiv.org/pdf/2005.00052.pdf">Pfeiffer et al. (2020)</a> and can be configured via the <code class="docutils literal notranslate"><span class="pre">inv_adapter</span></code> attribute of the <a class="reference internal" href="classes/adapter_config.html#transformers.AdapterConfig" title="transformers.AdapterConfig"><span class="xref myst py py-class"><code class="docutils literal notranslate"><span class="pre">AdapterConfig</span></code></span></a> class.</p>
<p><em>Example</em>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers.adapters</span> <span class="kn">import</span> <span class="n">PfeifferInvConfig</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">PfeifferInvConfig</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_adapter</span><span class="p">(</span><span class="s2">&quot;lang_adapter&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
<p><em>Papers:</em></p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/pdf/2005.00052.pdf">MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer</a> (Pfeiffer et al., 2020)</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>V1.x of adapter-transformers made a distinction between task adapters (without invertible adapters) and language adapters (with invertible adapters) with the help of the <code class="docutils literal notranslate"><span class="pre">AdapterType</span></code> enumeration.
This distinction was dropped with v2.x.</p>
</div>
</div>
<div class="section" id="prefix-tuning">
<h2>Prefix Tuning<a class="headerlink" href="#prefix-tuning" title="Permalink to this headline">¶</a></h2>
<p><em>Configuration class</em>: <a class="reference internal" href="classes/adapter_config.html#transformers.PrefixTuningConfig" title="transformers.PrefixTuningConfig"><span class="xref myst py py-class"><code class="docutils literal notranslate"><span class="pre">PrefixTuningConfig</span></code></span></a></p>
<div class="figure align-center" id="id2">
<a class="reference internal image-reference" href="_images/prefix.png"><img alt="Illustration of Prefix Tuning." src="_images/prefix.png" style="height: 300px;" /></a>
<p class="caption"><span class="caption-text">Illustration of the Prefix Tuning method within one Transformer layer. Trained components are colored in shades of magenta.</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</div>
<p>Prefix Tuning (<a class="reference external" href="https://aclanthology.org/2021.acl-long.353.pdf">Li and Liang, 2021</a>) introduces new parameters in the multi-head attention blocks in each Transformer layer.
More specifically, it prepends trainable prefix vectors <span class="math notranslate nohighlight">\(P^K\)</span> and <span class="math notranslate nohighlight">\(P^V\)</span> to the keys and values of the attention head input, each of a configurable prefix length <span class="math notranslate nohighlight">\(l\)</span> (<code class="docutils literal notranslate"><span class="pre">prefix_length</span></code> attribute):</p>
<div class="math notranslate nohighlight">
\[
head_i = \text{Attention}(Q W_i^Q, [P_i^K, K W_i^K], [P_i^V, V W_i^V])
\]</div>
<p>Following the original authors, the prefix vectors in <span class="math notranslate nohighlight">\(P^K\)</span> and <span class="math notranslate nohighlight">\(P^V\)</span> are not optimized directly but reparameterized via a bottleneck MLP.
This behavior is controlled via the <code class="docutils literal notranslate"><span class="pre">flat</span></code> attribute of the configuration.
Using <code class="docutils literal notranslate"><span class="pre">PrefixTuningConfig(flat=True)</span></code> will create prefix tuning vectors that are optimized without reparameterization.</p>
<p><em>Example</em>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers.adapters</span> <span class="kn">import</span> <span class="n">PrefixTuningConfig</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">PrefixTuningConfig</span><span class="p">(</span><span class="n">flat</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">prefix_length</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_adapter</span><span class="p">(</span><span class="s2">&quot;prefix_tuning&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
<p>As reparameterization using the bottleneck MLP is not necessary for performing inference on an already trained Prefix Tuning module, <code class="docutils literal notranslate"><span class="pre">adapter-transformers</span></code> includes a function to “eject” a reparameterized Prefix Tuning into a flat one:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">eject_prefix_tuning</span><span class="p">(</span><span class="s2">&quot;prefix_tuning&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>This will only retain the necessary parameters and reduces the size of the trained Prefix Tuning.</p>
<p><em>Papers:</em></p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/pdf/2101.00190.pdf">Prefix-Tuning: Optimizing Continuous Prompts for Generation</a> (Li and Liang, 2021)</p></li>
</ul>
</div>
<div class="section" id="compacter">
<h2>Compacter<a class="headerlink" href="#compacter" title="Permalink to this headline">¶</a></h2>
<p><em>Configuration class</em>: <a class="reference internal" href="classes/adapter_config.html#transformers.CompacterConfig" title="transformers.CompacterConfig"><span class="xref myst py py-class"><code class="docutils literal notranslate"><span class="pre">CompacterConfig</span></code></span></a>, <a class="reference internal" href="classes/adapter_config.html#transformers.CompacterPlusPlusConfig" title="transformers.CompacterPlusPlusConfig"><span class="xref myst py py-class"><code class="docutils literal notranslate"><span class="pre">CompacterPlusPlusConfig</span></code></span></a></p>
<div class="figure align-center" id="id3">
<a class="reference internal image-reference" href="_images/compacter.png"><img alt="Illustration of Compacter." src="_images/compacter.png" style="height: 300px;" /></a>
<p class="caption"><span class="caption-text">Illustration of the Compacter method within one Transformer layer. Trained components are colored in shades of magenta.</span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</div>
<p>The Compacter architecture proposed by <a class="reference external" href="https://arxiv.org/pdf/2106.04647.pdf">Mahabadi et al., 2021</a>
is similar to the bottleneck adapter architecture. It only exchanges the linear down- and
up-projection with a PHM layer. Unlike the linear layer, the PHM layer constructs its weight matrix from two smaller matrices, which reduces the number of parameters.
These matrices can be factorized and shared between all adapter layers. You can exchange the down- and up-projection layers from any of the bottleneck adapters described in the previous section
for a PHM layer by specifying <code class="docutils literal notranslate"><span class="pre">use_phm=True</span></code> in the config.</p>
<p>The PHM layer has the following additional properties: <code class="docutils literal notranslate"><span class="pre">phm_dim</span></code>, <code class="docutils literal notranslate"><span class="pre">shared_phm_rule</span></code>, <code class="docutils literal notranslate"><span class="pre">factorized_phm_rule</span></code>, <code class="docutils literal notranslate"><span class="pre">learn_phm</span></code>,
<code class="docutils literal notranslate"><span class="pre">factorized_phm_W</span></code>, <code class="docutils literal notranslate"><span class="pre">shared_W_phm</span></code>, <code class="docutils literal notranslate"><span class="pre">phm_c_init</span></code>, <code class="docutils literal notranslate"><span class="pre">phm_init_range</span></code>, <code class="docutils literal notranslate"><span class="pre">hypercomplex_nonlinearity</span></code></p>
<p>For more information, check out the <a class="reference internal" href="classes/adapter_config.html#transformers.AdapterConfig" title="transformers.AdapterConfig"><span class="xref myst py py-class"><code class="docutils literal notranslate"><span class="pre">AdapterConfig</span></code></span></a> class.</p>
<p>To add a Compacter to your model, you can use the predefined configs:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers.adapters</span> <span class="kn">import</span> <span class="n">CompacterConfig</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">CompacterConfig</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_adapter</span><span class="p">(</span><span class="s2">&quot;dummy&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
<p><em>Papers:</em></p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/pdf/2106.04647.pdf">COMPACTER: Efficient Low-Rank Hypercomplex Adapter Layers</a> (Mahabadi, Henderson and Ruder, 2021)</p></li>
</ul>
</div>
<div class="section" id="lora">
<h2>LoRA<a class="headerlink" href="#lora" title="Permalink to this headline">¶</a></h2>
<p><em>Configuration class</em>: <a class="reference internal" href="classes/adapter_config.html#transformers.LoRAConfig" title="transformers.LoRAConfig"><span class="xref myst py py-class"><code class="docutils literal notranslate"><span class="pre">LoRAConfig</span></code></span></a></p>
<div class="figure align-center" id="id4">
<a class="reference internal image-reference" href="_images/lora.png"><img alt="Illustration of LoRA." src="_images/lora.png" style="height: 300px;" /></a>
<p class="caption"><span class="caption-text">Illustration of the LoRA method within one Transformer layer. Trained components are colored in shades of magenta.</span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</div>
<p>Low-Rank Adaptation (LoRA) is an efficient fine-tuning technique proposed by <a class="reference external" href="https://arxiv.org/pdf/2106.09685.pdf">Hu et al. (2021)</a>.
LoRA injects trainable low-rank decomposition matrices into the layers of a pre-trained model.
For any model layer expressed as a matrix multiplication of the form <span class="math notranslate nohighlight">\(h = W_0 x\)</span>, it performs a reparameterization such that:</p>
<div class="math notranslate nohighlight">
\[
h = W_0 x + \frac{\alpha}{r} B A x
\]</div>
<p>Here, <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{r\times k}\)</span> and <span class="math notranslate nohighlight">\(B \in \mathbb{R}^{d\times r}\)</span> are the decomposition matrices and <span class="math notranslate nohighlight">\(r\)</span>, the low-dimensional rank of the decomposition, is the most important hyperparameter.</p>
<p>While, in principle, this reparameterization can be applied to any weight matrix in a model, the original paper only adapts the attention weights of the Transformer self-attention sub-layer with LoRA.
<code class="docutils literal notranslate"><span class="pre">adapter-transformers</span></code> additionally allows injecting LoRA into the dense feed-forward layers in the intermediate and output components of a Transformer block.
You can configure the locations where LoRA weights should be injected using the attributes in the <a class="reference internal" href="classes/adapter_config.html#transformers.LoRAConfig" title="transformers.LoRAConfig"><span class="xref myst py py-class"><code class="docutils literal notranslate"><span class="pre">LoRAConfig</span></code></span></a> class.</p>
<p><em>Example</em>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers.adapters</span> <span class="kn">import</span> <span class="n">LoRAConfig</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">LoRAConfig</span><span class="p">(</span><span class="n">r</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_adapter</span><span class="p">(</span><span class="s2">&quot;lora_adapter&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
<p>In the design of LoRA, Hu et al. (2021) also pay special attention to keeping the inference latency overhead compared to full fine-tuning at a minimum.
To accomplish this, the LoRA reparameterization can be merged with the original pre-trained weights of a model for inference.
Thus, the adapted weights are directly used in every forward pass without passing activations through an additional module.
In <code class="docutils literal notranslate"><span class="pre">adapter-transformers</span></code>, this can be realized using the built-in <a class="reference internal" href="classes/model_mixins.html#transformers.ModelAdaptersMixin.merge_adapter" title="transformers.ModelAdaptersMixin.merge_adapter"><span class="xref myst py py-meth"><code class="docutils literal notranslate"><span class="pre">merge_adapter()</span></code></span></a> method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">merge_adapter</span><span class="p">(</span><span class="s2">&quot;lora_adapter&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>To continue training on this LoRA adapter or to deactivate it entirely, the merged weights first have to be reset again:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">reset_adapter</span><span class="p">()</span>
</pre></div>
</div>
<p><em>Papers:</em></p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/pdf/2106.09685.pdf">LoRA: Low-Rank Adaptation of Large Language Models</a> (Hu et al., 2021)</p></li>
</ul>
</div>
<div class="section" id="ia-3">
<h2>(IA)^3<a class="headerlink" href="#ia-3" title="Permalink to this headline">¶</a></h2>
<p><em>Configuration class</em>: <a class="reference internal" href="classes/adapter_config.html#transformers.IA3Config" title="transformers.IA3Config"><span class="xref myst py py-class"><code class="docutils literal notranslate"><span class="pre">IA3Config</span></code></span></a></p>
<div class="figure align-center" id="id5">
<a class="reference internal image-reference" href="_images/ia3.png"><img alt="Illustration of (IA)^3." src="_images/ia3.png" style="height: 300px;" /></a>
<p class="caption"><span class="caption-text">Illustration of the (IA)^3 method within one Transformer layer. Trained components are colored in shades of magenta.</span><a class="headerlink" href="#id5" title="Permalink to this image">¶</a></p>
</div>
<p><em>Infused Adapter by Inhibiting and Amplifying Inner Activations ((IA)^3)</em> is an efficient fine-tuning method proposed within the <em>T-Few</em> fine-tuning approach by <a class="reference external" href="https://arxiv.org/pdf/2205.05638.pdf">Liu et al. (2022)</a>.
(IA)^3 introduces trainable vectors <span class="math notranslate nohighlight">\(l_W\)</span> into different components of a Transformer model, which perform element-wise rescaling of inner model activations.
For any model layer expressed as a matrix multiplication of the form <span class="math notranslate nohighlight">\(h = W x\)</span>, it therefore performs an element-wise multiplication with <span class="math notranslate nohighlight">\(l_W\)</span>, such that:</p>
<div class="math notranslate nohighlight">
\[
h = l_W \odot W x
\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\odot\)</span> denotes element-wise multiplication where the entries of <span class="math notranslate nohighlight">\(l_W\)</span> are broadcasted to the shape of <span class="math notranslate nohighlight">\(W\)</span>.</p>
<p><em>Example</em>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers.adapters</span> <span class="kn">import</span> <span class="n">IA3Config</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">IA3Config</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_adapter</span><span class="p">(</span><span class="s2">&quot;ia3_adapter&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
<p>The implementation of (IA)^3, as well as the <a class="reference internal" href="classes/adapter_config.html#transformers.IA3Config" title="transformers.IA3Config"><span class="xref myst py py-class"><code class="docutils literal notranslate"><span class="pre">IA3Config</span></code></span></a> class, are derived from the implementation of <a class="reference internal" href="#lora"><span class="std std-doc">LoRA</span></a>, with a few main modifications.
First, (IA)^3 uses multiplicative composition of weights instead of additive composition, as in LoRA.
Second, the added weights are not further decomposed into low-rank matrices.
These modifications are controlled via the <code class="docutils literal notranslate"><span class="pre">composition_mode</span></code> configuration attribute by setting <code class="docutils literal notranslate"><span class="pre">composition_mode=&quot;scale&quot;</span></code>.
Additionally, as the added weights are already of rank 1, <code class="docutils literal notranslate"><span class="pre">r=1</span></code> is set.</p>
<p>Beyond that, both methods share the same configuration attributes that allow you to specify which Transformer components rescaling vectors will be injected.
Following the original implementation, <a class="reference internal" href="classes/adapter_config.html#transformers.IA3Config" title="transformers.IA3Config"><span class="xref myst py py-class"><code class="docutils literal notranslate"><span class="pre">IA3Config</span></code></span></a> adds rescaling vectors to the self-attention weights (<code class="docutils literal notranslate"><span class="pre">selfattn_lora=True</span></code>) and the final feed-forward layer (<code class="docutils literal notranslate"><span class="pre">output_lora=True</span></code>).
Further, you can modify which matrices of the attention mechanism to rescale by leveraging the <code class="docutils literal notranslate"><span class="pre">attn_matrices</span></code> attribute.
By default, (IA)^3 injects weights into the key (‘k’) and value (‘v’) matrices but not in the query (‘q’) matrix.</p>
<p>Finally, similar to LoRA, (IA)^3 also allows merging the injected parameters with the original weight matrices of the Transformer model.
E.g.:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Merge (IA)^3 adapter</span>
<span class="n">model</span><span class="o">.</span><span class="n">merge_adapter</span><span class="p">(</span><span class="s2">&quot;ia3_adapter&quot;</span><span class="p">)</span>

<span class="c1"># Reset merged weights</span>
<span class="n">model</span><span class="o">.</span><span class="n">reset_adapter</span><span class="p">()</span>
</pre></div>
</div>
<p><em>Papers:</em></p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/pdf/2205.05638.pdf">Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning</a> (Liu et al., 2022)</p></li>
</ul>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="method_combinations.html" class="btn btn-neutral float-right" title="Method Combinations" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="overview.html" class="btn btn-neutral float-left" title="Overview and Configuration" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020-2022, Adapter-Hub Team

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  <!--- IMPORTANT: This file has modifications compared to the snippet on the documentation page! -->
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Versions</span>
    v: master
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Branches</dt>
      <dd><a href="methods.html">master</a></dd>
    </dl>
  </div>
</div>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>